---
title: 'Lab 4: Curvature, Fisher Information, and MLE CLT'
author: "Morgan Fissel 2000498470"
date: "2/5/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 0. Structure of this Lab
Section 1 explains the connection between curvature and variance. We will discuss the idea in the context that the model we are using for the data is correct. This assumption, is, of course, almost always wrong. There is a generic result that is beyond the scope of this class computationally. We will focus on getting curvature from numerical derivatives. Some of this is discussed in Section 4.4 of Statistical Models by A.C. Davison. If you are interested in the most generic statements, and feel brave, read Section 7.1 in Statistical Models by A.C. Davison.

Section 2 provides some `R` code for computing the Hessian, which is the curvature matrix. It also covers how to deal with results from one-dimensional with `optim` as well as `optimize` using the same Hessian function (`optimHess`). It will also talk about how to deal with transformations of variables multiple ways.

Section 3 lays out your tasks. Section 3.0 will discuss the Hessian in the context of the MLE for fitting a Gamma distribution to the data. Your tasks will be to analyze the same dataset using alternative parameterizations of the Gamma distribution.

For turning in the lab you must make the following modifications to this file.

1. Change the author field in the preamble above to your name followed by a space and then your IUID#  (all together in quotes so it is a string).

2. Rename the file by putting your last name in the file name. For instance, I would call my solution file `lab4_womack.Rmd`

3. DO NOT MODIFY Sections 0, 1, 2, and 3.0. Where there are tasks that require you to write an `R` chunk, do NOT delete the instructions, just place your chunk after the instructions for the task. Where there are questions that you need to answer, do NOT delete the questions, simply place your answer to a question in a new paragraph after the question is posed. If a task asks you to repeat procedures from previous tasks, you do not need to copy the intructions or questions from previous tasks, but you must repeat all of the procedures from the tasks completely and answer all of the questions in the new context. You can put all of your code for such a task into a single `R` chunk if doing so makes you happy.

# 1. Curvature and Variance
We will focus on two running examples before Section 3 of the lab. The two examples are the Gaussian (or Normal) distribution and the Exponential distribution. As a reminder, the densities of these distributions for a single observation are
\[
f(y;\mu,\sigma) = 
\frac{1}{\sqrt{2\pi\sigma^2}}
\exp\left(-\frac{(y-\mu)^2}{2\sigma^2}\right)
\]
for the Normal distribution ($\sigma$ must be positive) and
\[
f(x;\theta) = \theta\exp(-\theta x)
\]
for the exponential distribution ($\theta$ must be positive). Do not forget that the data under the Exponential distribution must be positive. So, these two distributions have different "support" - they both can't generate the same dataset because the Gaussian can generate any number, positive or negative, but the Exponential can only generate positive data. We will worry about issue like this more later. Also, there are two ways to parameterize the Exponential distribution. We have chosen the to use the rate instead of the mean. The mean is the reciprocal of the rate.

## 1.1 A Pretty Theoretical Bit
$\textbf{Caveat!}$ I suggest that as you read this bit that you do not focus on the details too much. This is here for filling out details from the text and to give you and idea about how arguments about these things are made. You can skip this bit, but I recommend looking at it to get a feeling for the underlying logic that is happening.

There will be two important connections about the quadratic forms we are getting. The first is the Central Limit Theorem for MLEs. The second is the that the test statistic we will look at next week, the Likeihood Ratio Test statistic. This statistic is asymptotically $\chi^2$, which we will talk about and use to form confidence intervals.

In `likelihoods.pdf`, we discussed some ideas relating to this convergence. We will further that work here. The observed data log-likelihood for iid data $y_1,\ldots,y_n$ generated by a distribution $F(\cdot;\theta)$ with density (mass) function $f(\cdot;\theta)$ is given by
\[
\ell\left(\theta;y_1,\ldots,y_n\right)
=
\sum_{i=1}^n \log\left(f\left(y_i;\theta\right)\right)
\]
and the maximum likelihood estimator is the $\widehat{\theta}$ that maximizes this
\[
\widehat{\theta} = 
\textrm{arg}\!\max_{\theta} \ell\left(\theta;y_1,\ldots,y_n\right)
\]
Let's call the true value of $\theta$ that generates the data $\theta$.
If we think about approximating $\ell(\theta;y_1,\ldots,y_n)$ as a quadratic near its mode $\widehat{\theta}$, then we get
\[
\begin{array}{rcl}
\ell\left(\theta;y_1,\ldots,y_n\right)
&\approx&
\ell\left(\widehat{\theta};y_1,\ldots,y_n\right)
+
\ell^\prime\left(\widehat{\theta};y_1,\ldots,y_n\right)
\left(\theta-\widehat{\theta}\right)
\\&&+
\frac{1}{2}\ell^{\prime\prime}\left(\widehat{\theta};y_1,\ldots,y_n\right)
\left(\theta-\widehat{\theta}\right)^2
\end{array}
\]
The middle term is $0$ because $\ell^\prime\left(\widehat{\theta};y_1,\ldots,y_n\right)=0$ because $\widehat{\theta}$ is the mode. So we get
\[
\ell\left(\theta;y_1,\ldots,y_n\right)-
\ell\left(\widehat{\theta};y_1,\ldots,y_n\right)
\approx
\frac{1}{2}\ell^{\prime\prime}\left(\widehat{\theta};y_1,\ldots,y_n\right)
\left(\theta-\widehat{\theta}\right)^2
\]
If we divide out the coefficient on $\left(\theta-\widehat{\theta}\right)^2$ we get
\[
\left(\theta-\widehat{\theta}\right)^2
\approx
2\frac{\ell\left(\theta;y_1,\ldots,y_n\right)-
\ell\left(\widehat{\theta};y_1,\ldots,y_n\right)
}{\ell^{\prime\prime}\left(\widehat{\theta};y_1,\ldots,y_n\right)}
\]
It looks like there is a connection between the squared error (and so also the variance) of $\widehat{\theta}$ and the curvature (which is given by the second derivative). But, it is not clear what should be happening with the numerator and there are random things on both sides, most unhelpful.

Let's try this whole approximation thing again but with the score. The score is the first derivative of the log-likelihood and given by
\[
u\left(\theta;y_1,\ldots,y_n\right) = \ell^\prime\left(\theta;y_1,\ldots,y_n\right)
\]
and we have $u\left(\widehat{\theta};y_1,\ldots,y_n\right)=0$. This lets us make the linear approximation for $u\left(\widehat{\theta};y_1,\ldots,y_n\right)$ around the (unknown) true value for $\theta$
\[
\begin{array}{rcl}
0&=&u\left(\widehat{\theta};y_1,\ldots,y_n\right)
\approx
u\left(\theta;y_1,\ldots,y_n\right)
+
u^\prime\left(\theta;y_1,\ldots,y_n\right)
\left(\theta-\widehat{\theta}\right)
\\&=&
u\left(\theta;y_1,\ldots,y_n\right)
+
\ell^{\prime\prime}\left(\theta;y_1,\ldots,y_n\right)
\left(\theta-\widehat{\theta}\right)
\end{array}
\]
If we solve for $\widehat{\theta}-\theta$, we get
\[
\left(\widehat{\theta}-\theta\right)
\approx
\frac{
u\left(\theta;y_1,\ldots,y_n\right)
}{
\ell^{\prime\prime}\left(\theta;y_1,\ldots,y_n\right)
}
=\frac{
\ell^\prime\left(\theta;y_1,\ldots,y_n\right)
}{
\ell^{\prime\prime}\left(\theta;y_1,\ldots,y_n\right)
}
\]
If we square both sides, we get
\[
\left(\widehat{\theta}-\theta\right)^2
\approx
\frac{
\left(\ell^\prime\left(\theta;y_1,\ldots,y_n\right)\right)^2
}{
\left(
\ell^{\prime\prime}\left(\theta;y_1,\ldots,y_n\right)
\right)^2
}
\]
We once again see that there is a clear connection between curvature and squared error (and thus variance). The only randomness on the right hand side comes from the fact that we have sums for $\ell^\prime$ and $\ell^{\prime\prime}$. But we know how to deal with sums, we divide by $n$ make them averages and invoke the LLN or the CLT. First, we will deal with the denominator. We have
\[
\frac{\ell^{\prime\prime}\left(\theta;y_1,\ldots,y_n\right)}{n}
\stackrel{P}{\rightarrow}
\textrm{E}\left[
\frac{\partial^2}{\partial\theta^2}
\log\left(f(Y;\theta)\right)
\right]
\]
When multiplied by negative one, the theoretical quantity on the right hand side is called the Fisher information for one observation and often denoted by $I_0(\theta)$.
Doing a bit of rearranging and invoking Slutsky's Theorem, we get
\[
\left(\widehat{\theta}-\theta\right)^2
\approx
\frac{
\left(\ell^\prime\left(\theta;y_1,\ldots,y_n\right)\right)^2
}{
n^2\left(
\frac{\ell^{\prime\prime}\left(\theta;y_1,\ldots,y_n\right)}{n}
\right)^2
}
\approx
\frac{1}{n}
\frac{
\left(\ell^\prime\left(\theta;y_1,\ldots,y_n\right)\right)^2/n
}{
(-I_0(\theta))^2
}
\]
We are almost at the end! Taking expectations on both sides, we get
\[
\begin{array}{rcl}
\textrm{E}\left[\left(\widehat{\theta}-\theta\right)^2\right]
&\approx&
\frac{1}{nI_0(\theta)^2}
\textrm{E}\left[
\frac{\left(\ell^\prime\left(\theta;y_1,\ldots,y_n\right)\right)^2}{n}
\right]
\\&=&
\frac{1}{nI_0(\theta)^2}
\textrm{E}\left[
\frac{\left(\frac{\partial}{\partial\theta}\sum_{i=1}^n \log(f(y_i;\theta))\right)^2}{n}
\right]
\\&=&
\frac{1}{nI_0(\theta)^2}
\textrm{E}\left[
\frac{\left(\sum_{i=1}^n \frac{\partial}{\partial\theta}\log(f(y_i;\theta))\right)^2}{n}
\right]
\\&=&
\frac{1}{nI_0(\theta)^2}
\textrm{E}\left[
\frac{\sum_{i=1}^n \left(\frac{\partial}{\partial\theta}\log(f(y_i;\theta))\right)^2}{n}
\right] \quad\text{because of independence}
\\&=&
\frac{1}{nI_0(\theta)^2}
\textrm{E}\left[
\left(\frac{\partial}{\partial\theta}\log(f(Y;\theta))\right)^2
\right] \quad\text{because of identially distributed}
\end{array}
\]
This new quantity, $\textrm{E}\left[
\left(\frac{\partial}{\partial\theta}\log(f(Y;\theta))\right)^2
\right]$
is usually denoted by
$K_0(\theta)$ (or $J_0(\theta)$, though the book uses $J$ for observed Fisher information) and is another way to measure information. It turns out that when $f(\cdot;\theta)$ is the correct model and we have satisfied some minor conditions then
$K_0(\theta) = I_0(\theta)$. Because of this, $K_0(\theta)$ is often also called the Fisher Information, though it is usually referred to as the variance of the score function in order to distinguish it from $I_0(\theta)$.

FINALLY, using all of this, we get
\[
\textrm{E}\left[\left(\widehat{\theta}-\theta\right)^2\right]
\approx
\frac{1}{nI_0(\theta)}
\]
and we can clearly see the connection between squared error (and thus variance) and curvature (the Fisher Information). We can compute the theoretical Fisher Information and plug in our MLE or we can just make the computer calculate the curvature for us.

## 1.2 The MLE Central Limit Theorem
Now we get to the real purpose here. We have
\[
\textrm{E}\left[\left(\widehat{\theta}-\theta\right)^2\right]
\approx
\frac{1}{nI_0(\theta)}
\]
where 
\[
I_0(\theta) = -\textrm{E}\left[\frac{\partial^2}{\partial\theta^2}\log(f(Y;\theta))\right]
\]
is the Fisher Information for one observation (multiplying by $n$ provides the Fisher Information for $n$ observations).
Because the right hand side in the approximation for $\textrm{E}\left[\left(\widehat{\theta}-\theta\right)^2\right]$ is decreasing like $1/n$ we can easily first conclude the Law of Large Numbers
\[
\widehat{\theta}
\stackrel{P}{\rightarrow}
\theta
\]
If we divide $\left(\widehat{\theta}-\theta\right)$ by 
$\sqrt{nI_0(\theta)}$, then we get the CLT for MLEs
\[
\sqrt{nI_0(\theta)}\left(\widehat{\theta}-\theta\right)
\stackrel{D}{\rightarrow}
Z \sim N(0,1)
\]
and $nI_0(\theta)$ is the precision, which is the reciprocal of the variance. So curvature tells us precision and the reciprocal of the curvature tells us variance. Of course, we do not know $\theta$ and so we can plug in the MLE to approximate the precision and get a similar CLT statement by taking advantage of Slutsky's Theorem
\[
\sqrt{nI_0\left(\widehat{\theta}\right)}\left(\widehat{\theta}-\theta\right)
\stackrel{D}{\rightarrow}
Z \sim N(0,1)
\]
One last little change is that maybe we do not want to do (or cannot do) the theoretical integral by hand. Of course, we do not have to and can instead just use the curvature from the sample average as an estimate
\[
nI_0\left(\widehat{\theta}\right)
\approx
-\ell^{\prime\prime}\left(\widehat{\theta};
y_1,\ldots,y_n\right)
\]
We get the final CLT statement
\[
\sqrt{
-\ell^{\prime\prime}\left(\widehat{\theta};
y_1,\ldots,y_n\right)
}\left(\widehat{\theta}-\theta\right)
\stackrel{D}{\rightarrow}
Z \sim N(0,1)
\]
This super useful because it lets us compute confidence intervals in exactly the same way that you did for the sample mean in Stat-S350
\[
\text{CI} = \widehat{\theta}\pm 
\frac{z}{\sqrt{
-\ell^{\prime\prime}\left(\widehat{\theta};
y_1,\ldots,y_n\right)}
}
\]
where $z$ is the appropriate z-score for whatever confidence level you want. 
Two notes: (1) Because we have been invoking limits, this all only works as an approximation for large sample sizes. Something like 100 is usually enough and even as few as 35 or 40 can be sufficient for the approximation to be good. (2) Some parameterizations provide quicker approximations by normal distributions than others. One such transformation is called a Variance Stabilizing Transformation. This is a Stat 420 topic and we will not discuss it this course.

## 1.3 Our Examples
The Gaussian density for iid observed data $y_1,\ldots,y_n$ is
\[
f(y_1,\ldots,y_n;\mu,\sigma) = \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^{n}
\exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i-\mu)^2\right)
\]
The log-likelihood of the observed data is
\[
\ell(\mu,\sigma;y_1,\ldots,y_n) = -\frac{n}{2}\log\left(2\pi\sigma^2\right)
-\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i-\mu)^2
\]
The MLE for $\mu$ whether we assume that $\sigma^2$ is known is given by $\hat{\mu}=\bar{y}$, the sample mean. When $\sigma^2$ is unknown, its MLE is $\hat{\sigma}^2=\sum (y_i-\bar{y})^2/n$. When $\sigma is known, the second derivative of the likelihood is given by
\[
\frac{\partial^2}{\partial\mu^2}\ell(\mu,\sigma;y_1,\ldots,y_n)
=
-\frac{n}{\sigma^2}
\]
and so $I_0(\theta) = \frac{1}{\sigma^2}$ and the variance of the MLE is
\[
\textrm{Var}\left[\hat{\mu}\right] = \frac{\sigma^2}{n}
\]
If we treated $\sigma$ as unknown, we would get a Hessian matrix of second derivatives, which is
\[
H(\mu,\sigma;y_1,\ldots,y_n) = -\left(
\begin{array}{cc}
\frac{n}{\sigma^2} & \frac{n(\mu-\bar{y})}{\sigma^2}\\
 \frac{n(\mu-\bar{y})}{\sigma^2} & -\frac{n}{\sigma^2} + \frac{3\sum (y_i-\mu)^2}{\sigma^4}
 \end{array}
\right)
\]
Taking expectations would give
\[
\textrm{E}\left[H(\mu,\sigma;y_1,\ldots,y_n)\right] = -\left(
\begin{array}{cc}
\frac{n}{\sigma^2} & 0\\
0 & \frac{2n}{\sigma^2}
\end{array}
\right)
\]
If we take the inverse of this matrix we get the variance-covariance matrix. If we look at the $(1,1)$ element, we get $\sigma^2/n$, the same variance for the sample mean as before and we could plug in an estimte of $\sigma^2$ if we wanted to. Alternatively, we could have plugged in the MLEs and gotten
\[
H\left(\hat{\mu},\hat{\sigma};y_1,\ldots,y_n\right) = -\left(
\begin{array}{cc}
\frac{n}{\hat{\sigma}^2} & 0\\
0 & \frac{2n}{\hat{\sigma}^2}
\end{array}
\right)
\]
and we would get the same estimate of the variance for $\mu$, $\sigma^2/n$.

Now for the exponential distribution. We have that the density for the data $x_1,\ldots,x_n$
\[
f(x_1,\ldots,x_n;\theta) = \theta^n\exp\left(-\sum\theta x_i\right)
\]
The log_likelihood is
\[
\ell(\theta;x_1,\ldots,x_n) = n\log(\theta)-n\theta \bar{x}
\]
and the MLE is $\hat{\theta} = \frac{1}{\bar{x}}$. The second derivative of the log-likelihood is
\[
\ell^{\prime\prime}(\theta;x_1,\ldots,x_n) = -\frac{n}{\theta^2}
\]
and so $I_0(\theta) = 1/\theta^2$. The variance of the MLE is given by $\theta^2/n$ and we could plug in $\hat{\theta}$ in order to get an estimate of it.


# 2. Computing the Hessian
We first do the computation for the two dimensional case and then for the one dimensional case.

## 2.1 Normal Distribution (Archetype for parameter dimension>1)
Let's make a workflow of things we need to do.
```{r}
#standard log-likelihood function
loglik_fun_norm = function(theta,y){
  mu = theta[1]
  sigma = theta[2]
  sum(dnorm(y,mu,sigma,log=TRUE))
}
#log-likelihood for sigma=exp(theta[2])
loglik_fun_norm_trans = function(theta,y){
  mu = theta[1]
  sigma = exp(theta[2])
  sum(dnorm(y,mu,sigma,log=TRUE))
}
#first we generate some random data
y = rnorm(100,mean=10,sd=3)
#using optim to get MLE
#we use the _trans function and then undo the transformation for the estimate without the transformation
fit_norm_trans = optim(c(0,0),loglik_fun_norm_trans,y=y,control=list(fnscale=-1))
fit_norm = fit_norm_trans
fit_norm$par[2] = exp(fit_norm$par[2])
#using optimHess to get the Hessian
#we use the regular log-likelihood function
fit_norm_hess = optimHess(fit_norm$par,loglik_fun_norm,y=y)
#we convert the Hessian into a variance-covariance matrix
#inversion is achieved using solve
fit_norm_vcov = solve(-fit_norm_hess)
#let's comppare the first element of this to the MLE for sigma^2 divided by n
c(fit_norm_vcov[1,1],fit_norm$par[2]^2/length(y))
```

Let's do that all again, but wrapped in an MLE function that returns th `vcov` matrix if we want it to
```{r}
mle_norm = function(y,vcov=TRUE){
  #takes advantage of scoping
  loglik_fun = function(theta){#scoping, looks for y one environment up
    mu = theta[1]
    sigma = theta[2]
    sum(dnorm(y,mu,sigma,log=TRUE))
  }
  #log-likelihood for sigma=exp(theta[2])
  loglik_fun_trans = function(theta){#scoping, looks for y one environment up
    mu = theta[1]
    sigma = exp(theta[2])
    sum(dnorm(y,mu,sigma,log=TRUE))
  }
  #using optim to get MLE
  #we use the _trans function and then undo the transformation for the estimate without the transformation
  fit_trans = optim(c(0,0),loglik_fun_trans,control=list(fnscale=-1)) #scoping, looks for y one environment up
  fit = fit_trans
  fit$par[2] = exp(fit$par[2])
  names(fit$par) = c("mu","sigma") #NAMES ARE GOOD
  if(!vcov){
    return(list(theta=fit$par,loglik=fit$value))
  }else{
    #using optimHess to get the Hessian
    #we use the regular log-likelihood function
    fit_hess = optimHess(fit$par,loglik_fun)#scoping, looks for y one environment up
    #we convert the Hessian into a variance-covariance matrix
    #inversion is achieved using solve
    fit_vcov = solve(-fit_hess)
    rownames(fit_vcov) = colnames(fit_vcov) = names(fit$par)#NAMES ARE COPIED HERE
    return(list(theta=fit$par,loglik=fit$value,vcov=fit_vcov))
  }
}

#test it out
mle_norm(y)
```

## 2.1 Exponential Distribution (Archetype for parameter dimension=1)
```{r}
#standard log-likelihood function
loglik_fun_exp = function(theta,y){
  sum(dexp(y,rate=theta,log=TRUE))
}
#first we generate some random data
x = rexp(1000,rate=4)
#using optimize to get MLE because it is easier
#we use the regular log-likelihood function
fit_exp = optimize(loglik_fun_exp,c(1e-10,1e5),y=x,maximum=TRUE)
#using optimHess to get the Hessian
#we use the regular log-likelihood function
fit_exp_hess = optimHess(fit_exp$maximum,loglik_fun_exp,y=x)[1,1] #1 by 1 matrix - accessing element to get a scalar
#we convert the Hessian into a variance-covariance matrix
#inversion is achieved using solve
fit_exp_vcov = -1/fit_exp_hess
#let's comppare the first element of this to the MLE for theta^2 divided by n
c(fit_exp_vcov,fit_exp$maximum[1]^2/length(x))
```

Lets do that all again, but wrapped in an MLE function that returns th `vcov` matrix if we want it to
```{r}
mle_exp = function(y,vcov=TRUE){
  #takes advantage of scoping
  loglik_fun = function(theta){#scoping, looks for y one environment up
    sum(dexp(y,rate=theta,log=TRUE))
  }
  #using optimize to get MLE
  #we use the regular log-likelihood function
  fit = optimize(loglik_fun,c(1e-10,1e5),maximum=TRUE) #scoping, looks for y one environment up
  names(fit$maximum) = "theta" #NAMES ARE GOOD
  if(!vcov){
    return(list(theta=fit$maximum,loglik=fit$objective))
  }else{
    #using optimHess to get the Hessian
    #we use the regular log-likelihood function
    fit_hess = optimHess(fit$maximum,loglik_fun)[1,1] #1 by 1 matrix - accessing element to get a scalar\
    #scoping, looks for y one environment up
    #we convert the Hessian into a variance-covariance matrix
    #inversion is achieved using division
    fit_vcov = -1/fit_hess
    names(fit_vcov) = names(fit$maximum)#NAMES ARE COPIED HERE
    return(list(theta=fit$maximum,loglik=fit$objective,vcov=fit_vcov))
  }
}

#test it out
mle_exp(x)
```


# 3. Tasks
Before we get started, lets read in a dataset to analyze. 
```{r}
data("faithful")
y = faithful$eruptions[faithful$waiting>71]
```

##  3.0 Hessian from Gamma Distribution using shape and rate
We are going to code up the MLE function for the Gamma distribution and have it return the variance-covariance matrix. 
For this example, I use the parameterization using a shape parameter $\alpha$ and a rate parameter $\beta$. Both parameters must be positive. The density for a single observation is given by
\[
f(y;\alpha,\beta)
=
\frac{\beta^\alpha}{\Gamma(\alpha)}
y^{\alpha-1}\exp\left(-\beta y\right)
\]

The MLE function with possible `vcov` return is
```{r}
mle_gamma_shape_rate = function(y,vcov=TRUE){
  #takes advantage of scoping
  loglik_fun = function(theta){#scoping, looks for y one environment up
    alpha = theta[1]
    beta = theta[2]
    sum(dgamma(y,shape=alpha,rate=beta,log=TRUE))
  }
  loglik_fun_trans = function(theta){#scoping, looks for y one environment up
    alpha = exp(theta[1])
    beta = exp(theta[2])
    sum(dgamma(y,shape=alpha,rate=beta,log=TRUE))
  }
  #using optim to get MLE
  #we use the _trans function and then undo the transformation for the estimate without the transformation
  fit_trans = optim(c(0,0),loglik_fun_trans,control=list(fnscale=-1)) #scoping, looks for y one environment up
  fit = fit_trans
  fit$par[1] = exp(fit$par[1])
  fit$par[2] = exp(fit$par[2])
  names(fit$par) = c("alpha","beta") #NAMES ARE GOOD
  if(!vcov){
    return(list(theta=fit$par,loglik=fit$value))
  }else{
    #using optimHess to get the Hessian
    #we use the regular log-likelihood function
    fit_hess = optimHess(fit$par,loglik_fun)#scoping, looks for y one environment up
    #we convert the Hessian into a variance-covariance matrix
    #inversion is achieved using solve
    fit_vcov = solve(-fit_hess)
    rownames(fit_vcov) = colnames(fit_vcov) = names(fit$par)#NAMES ARE COPIED HERE
    return(list(theta=fit$par,loglik=fit$value,vcov=fit_vcov))
  }
}

```

Plotting!
```{r}
#fitting model
fit_shape_rate = mle_gamma_shape_rate(y) 
print(fit_shape_rate)

#plotting
#let's just see if the density looks okay (spoiler alert, it does not)
plot(density(y))
y_vals_plot = seq(min(y),max(y),length.out = 1000)
dens_fitted_plot = dgamma(y_vals_plot,
                          shape=fit_shape_rate$theta["alpha"],
                          rate=fit_shape_rate$theta["beta"]
                          )
lines(dens_fitted_plot~y_vals_plot,lty=2,col="blue")
```

We can form an asymptotic $95\%$ confidence interval for the shape parameter using the output variance covariance matrix and the output MLE.
```{r}
est_alpha = fit_shape_rate$theta["alpha"]
var_est_alpha = fit_shape_rate$vcov["alpha","alpha"]
sd_est_alpha = sqrt(var_est_alpha)
ci_alpha = est_alpha + c(-1,1)*sd_est_alpha*qnorm(0.975)
print(ci_alpha)
```

We can also form an asymptotic $95\%$ confidence interval for the rate parameter using the output variance covariance matrix and the output MLE.
```{r}
est_beta = fit_shape_rate$theta["beta"]
var_est_beta = fit_shape_rate$vcov["beta","beta"]
sd_est_beta = sqrt(var_est_beta)
ci_beta = est_beta + c(-1,1)*sd_est_beta*qnorm(0.975)
print(ci_beta)
```

##  3.1 Task 1
There is an alternative parameterization of the Gamma distribution in terms of the shape parameter $\alpha$ and a scale parameter $\lambda$. The relationship between the scale $\lambda$ and the rate $\beta$ is $\lambda=1/\beta$. Write an MLE function similar to that from Task 0 that fits the gamma distribution under this alternative parameterization. Fit the data `y` using this distributions and get a $95\%$ confidence intervals for the shape and scale parameters.
```{r}
mle_gamma_shape_scale = function(y,vcov=TRUE){
  #takes advantage of scoping
  loglik_fun = function(theta){#scoping, looks for y one environment up
    alpha = theta[1]
    lambda = theta[2]
    beta = 1/lambda
    sum(dgamma(y,shape=alpha,rate=beta,log=TRUE))
  }
  loglik_fun_trans = function(theta){#scoping, looks for y one environment up
    alpha = exp(theta[1])
    lambda = exp(theta[2])
    beta = 1/lambda
    sum(dgamma(y,shape=alpha,rate=beta,log=TRUE))
  }
  #using optim to get MLE
  #we use the _trans function and then undo the transformation for the estimate without the transformation
  fit_trans = optim(c(0,0),loglik_fun_trans,control=list(fnscale=-1)) #scoping, looks for y one environment up
  fit = fit_trans
  fit$par[1] = exp(fit$par[1])
  fit$par[2] = exp(fit$par[2])
  names(fit$par) = c("alpha","lambda") #NAMES ARE GOOD
  if(!vcov){
    return(list(theta=fit$par,loglik=fit$value))
  }else{
    #using optimHess to get the Hessian
    #we use the regular log-likelihood function
    fit_hess = optimHess(fit$par,loglik_fun)#scoping, looks for y one environment up
    #we convert the Hessian into a variance-covariance matrix
    #inversion is achieved using solve
    fit_vcov = solve(-fit_hess)
    rownames(fit_vcov) = colnames(fit_vcov) = names(fit$par)#NAMES ARE COPIED HERE
    return(list(theta=fit$par,loglik=fit$value,vcov=fit_vcov))
  }
}

fit_shape_scale = mle_gamma_shape_scale(y) 
print(fit_shape_scale)
print(fit_shape_rate)

est_alpha = fit_shape_scale$theta["alpha"]
var_est_alpha = fit_shape_scale$vcov["alpha","alpha"]
sd_est_alpha = sqrt(var_est_alpha)
ci_alpha = est_alpha + c(-1,1)*sd_est_alpha*qnorm(0.95)
print(ci_alpha)

est_lambda = fit_shape_scale$theta["lambda"]
var_est_lambda = fit_shape_scale$vcov["lambda","lambda"]
sd_est_lambda = sqrt(var_est_lambda)
ci_lambda = est_lambda + c(-1,1)*sd_est_lambda*qnorm(0.95)
print(ci_lambda)

plot(density(y))
y_vals_plot = seq(min(y),max(y),length.out = 1000)
dens_fitted_plot = dgamma(y_vals_plot,shape=fit_shape_scale$theta["alpha"],scale=fit_shape_scale$theta["lambda"])
lines(dens_fitted_plot~y_vals_plot,lty=2,col="blue")
```

##  3.2 Task 2
One last parameterization of the Gamma distribution is in terms of the shape parameter $\alpha$ and a mean parameter $\mu$. As a function of the shape $\alpha$ and rate $\beta$ the mean $\mu$ is given by $\mu=\alpha/\beta$.  As a function of the shape $\alpha$ and scale $\lambda$ the mean $\mu$ is given by $\mu=\alpha\lambda$. Write an MLE function similar to those from Task 0 and Task 1 that fits the gamma distribution under this alternative parameterization. Fit the data `y` using this distributions and get a $95\%$ confidence intervals for the shape and mean parameters.
```{r}
mle_gamma_shape_mean = function(y,vcov=TRUE){
  #takes advantage of scoping
  loglik_fun = function(theta){#scoping, looks for y one environment up
    alpha = theta[1]
    mean = theta[2]
    beta = alpha/mean
    sum(dgamma(y,shape=alpha,rate=beta,log=TRUE))
  }
  loglik_fun_trans = function(theta){#scoping, looks for y one environment up
    alpha = exp(theta[1])
    mean = exp(theta[2])
    beta = alpha/mean
    sum(dgamma(y,shape=alpha,rate=beta,log=TRUE))
  }
  #using optim to get MLE
  #we use the _trans function and then undo the transformation for the estimate without the transformation
  fit_trans = optim(c(0,0),loglik_fun_trans,control=list(fnscale=-1)) #scoping, looks for y one environment up
  fit = fit_trans
  fit$par[1] = exp(fit$par[1])
  fit$par[2] = exp(fit$par[2])
  names(fit$par) = c("alpha","mean") #NAMES ARE GOOD
  if(!vcov){
    return(list(theta=fit$par,loglik=fit$value))
  }else{
    #using optimHess to get the Hessian
    #we use the regular log-likelihood function
    fit_hess = optimHess(fit$par,loglik_fun)#scoping, looks for y one environment up
    #we convert the Hessian into a variance-covariance matrix
    #inversion is achieved using solve
    fit_vcov = solve(-fit_hess)
    rownames(fit_vcov) = colnames(fit_vcov) = names(fit$par)#NAMES ARE COPIED HERE
    return(list(theta=fit$par,loglik=fit$value,vcov=fit_vcov))
  }
}

fit_shape_mean = mle_gamma_shape_mean(y) 
print(fit_shape_mean)

est_alpha = fit_shape_mean$theta["alpha"]
var_est_alpha = fit_shape_mean$vcov["alpha","alpha"]
sd_est_alpha = sqrt(var_est_alpha)
ci_alpha = est_alpha + c(-1,1)*sd_est_alpha*qnorm(0.95)
print(ci_alpha)

est_mean = fit_shape_mean$theta["mean"]
var_est_mean = fit_shape_mean$vcov["mean","mean"]
sd_est_mean = sqrt(var_est_mean)
ci_mean = est_mean + c(-1,1)*sd_est_mean*qnorm(0.95)
print(ci_mean)

plot(density(y))
y_vals_plot = seq(min(y),max(y),length.out = 1000)
dens_fitted_plot = dgamma(y_vals_plot,shape=fit_shape_mean$theta["alpha"],
                          rate=((fit_shape_mean$theta["alpha"])/(fit_shape_mean$theta["mean"])))
lines(dens_fitted_plot~y_vals_plot,lty=2,col="red")
```

##  3.3 Task 3
An alternative distribution for modeling positive data is the Weibull distribution. It also has two parameters, a shape $k>0$ and a scale $\lambda>0$. The density function for a single observation is
\[
f(y;k,\lambda)
=
\frac{k}{\lambda^k}y^{k-1}\exp\left(-\left(\frac{y}{\lambda}\right)^k\right)
\]
When $k>1$ this has a tighter tail than the Gamma distribution and might fit this dataset better. There is a `dweibull` function in `R` whose call might be something like `dweibull(y,shape=k,scale=lambda,log=[TRUE/FALSE])`.

Write an MLE function similar to the one from Task 0 for the Weibull model. Fit the Weibull model to the data. Plot the fitted density with the density plot of the data. Report confidence intervals for $k$ and $\lambda$.
```{r}
mle_weibull_shape_scale = function(y,vcov=TRUE){
  #takes advantage of scoping
  loglik_fun = function(theta){#scoping, looks for y one environment up
    k = theta[1]
    lambda = theta[2]
    sum(dweibull(y,shape=k,scale=lambda,log=TRUE))
  }
  loglik_fun_trans = function(theta){#scoping, looks for y one environment up
    k = exp(theta[1])
    lambda = exp(theta[2])
    sum(dweibull(y,shape=k,scale=lambda,log=TRUE))
  }
  #using optim to get MLE
  #we use the _trans function and then undo the transformation for the estimate without the transformation
  fit_trans = optim(c(0,0),loglik_fun_trans,control=list(fnscale=-1)) #scoping, looks for y one environment up
  fit = fit_trans
  fit$par[1] = exp(fit$par[1])
  fit$par[2] = exp(fit$par[2])
  names(fit$par) = c("k","lambda") #NAMES ARE GOOD
  if(!vcov){
    return(list(theta=fit$par,loglik=fit$value))
  }else{
    #using optimHess to get the Hessian
    #we use the regular log-likelihood function
    fit_hess = optimHess(fit$par,loglik_fun)#scoping, looks for y one environment up
    #we convert the Hessian into a variance-covariance matrix
    #inversion is achieved using solve
    fit_vcov = solve(-fit_hess)
    rownames(fit_vcov) = colnames(fit_vcov) = names(fit$par)#NAMES ARE COPIED HERE
    return(list(theta=fit$par,loglik=fit$value,vcov=fit_vcov))
  }
}

fit_shape_scale_w = mle_weibull_shape_scale(y)

est_k = fit_shape_scale_w$theta["k"]
var_est_k = fit_shape_scale_w$vcov["k","k"]
sd_est_k = sqrt(var_est_k)
ci_k = est_k + c(-1,1)*sd_est_k*qnorm(0.95)
print(ci_k)

est_lambda = fit_shape_scale_w$theta["lambda"]
var_est_lambda = fit_shape_scale_w$vcov["lambda","lambda"]
sd_est_lambda = sqrt(var_est_lambda)
ci_lambda = est_lambda + c(-1,1)*sd_est_lambda*qnorm(0.95)
print(ci_lambda)

plot(density(y),ylim=c(0,1.3))
y_vals_plot = seq(min(y),max(y),length.out = 1000)
dens_fitted_plot = dweibull(y_vals_plot,shape=fit_shape_scale_w$theta["k"],scale=fit_shape_scale_w$theta["lambda"])
lines(dens_fitted_plot~y_vals_plot,lty=2,col="purple")
```

##  3.4 Task 4
You have fit the Gamma model under different parameterizations and you have also fit a Weibull model to the data. Which model do you like better for understanding this dataset? Why?
I like the weibull model, it seems to capture the data the best. The curve is fairly similar to the KDE just more smooth and a little taller. While the other models seem to still be pretty similar to the gamma(probably because they are all gamma dist.).
